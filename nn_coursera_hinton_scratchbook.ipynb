{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectures 1+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear neurons**\n",
    "\n",
    "$y=b+\\sum_ix_iw_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(x) is (100,) and shape(w) is (100,)\n",
      "25.3055298596 ()\n"
     ]
    }
   ],
   "source": [
    "def linear_neuron(x,w,b = 0):\n",
    "    \"\"\"\n",
    "    x = (m*1)\n",
    "    w = (m*1)\n",
    "    b = scalar\n",
    "    \"\"\"\n",
    "    assert (len(x.shape) ==1) & (len(w.shape) == 1) & (x.shape[0] == w.shape[0])\n",
    "    print(\"shape(x) is %s and shape(w) is %s\" %(str(x.shape),str(w.shape)))\n",
    "    y = b + np.dot(w.T,x)\n",
    "    return y\n",
    "\n",
    "x = np.random.rand(100,)\n",
    "w = np.random.rand(100,)\n",
    "y = linear_neuron(x,w)\n",
    "\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(x) is (100,) and shape(w) is (100,)\n",
      "z is 25.651\n",
      "y is 0.000\n"
     ]
    }
   ],
   "source": [
    "def binary_threshold_neuron(x,w,b = 0, theta = None):\n",
    "    \"\"\"\n",
    "    x = (m*1)\n",
    "    w = (m*1)\n",
    "    b = scalar\n",
    "    \"\"\"\n",
    "    assert (len(x.shape) ==1) & (len(w.shape) == 1) & (x.shape[0] == w.shape[0])\n",
    "    if theta == None: theta = -b\n",
    "    print(\"shape(x) is %s and shape(w) is %s\" %(str(x.shape),str(w.shape)))\n",
    "    z = b + np.dot(w.T,x)\n",
    "    print(\"z is %.3f\" % z)\n",
    "    y = 1 if (z >= theta) else 0\n",
    "    return y\n",
    "\n",
    "x = np.random.rand(100,)\n",
    "w = np.random.rand(100,)\n",
    "y = binary_threshold_neuron(x,w, b=1, theta = 60)\n",
    "\n",
    "print(\"y is %.3f\" % y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(x) is (100,) and shape(w) is (100,)\n",
      "z is 0.026\n",
      "y is 0.5063841208407469096641762\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_neuron(x,w,b = 0.):\n",
    "    \"\"\"\n",
    "    x = (m*1)\n",
    "    w = (m*1)\n",
    "    b = scalar\n",
    "    \"\"\"\n",
    "    assert (len(x.shape) ==1) & (len(w.shape) == 1) & (x.shape[0] == w.shape[0])\n",
    "    print(\"shape(x) is %s and shape(w) is %s\" %(str(x.shape),str(w.shape)))\n",
    "    z = b + np.dot(w.T,x)\n",
    "    print(\"z is %.3f\" % z)\n",
    "    y = 1. / (1. + np.exp(-z))\n",
    "    return y\n",
    "\n",
    "x = np.random.rand(100,)\n",
    "w = np.random.rand(100,) * np.full(100,.001) #make weights small scale\n",
    "y = sigmoid_neuron(x,w)\n",
    "\n",
    "print(\"y is %.25f\" % y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUQy0P8HUutVd310rNIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x843cac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_sigmoid():\n",
    "    z = np.arange(-10,10,.02)\n",
    "    y = 1. / (1. + np.exp(-z))\n",
    "    plt.plot(z,y)\n",
    "    plt.show()\n",
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Angle between vectors**\n",
    "\n",
    "$A.B = |A|.|B|.cos(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45.000000000000007, 5)\n",
      "(90.0, 0.0)\n",
      "(105.25511870305778, -0.59999999999999998)\n",
      "Multidimentional vectors [ 0.4933704   0.42779018  0.33321147  0.6816322   0.95980062] [ 0.90039248  0.34193319  0.62649577  0.90058925  0.27429217]\n",
      "(35.532526599242992, 1.6763946625219732)\n"
     ]
    }
   ],
   "source": [
    "(def get_angle_dot_vectors(v1,v2):\n",
    "    \"\"\"\n",
    "    Returns the angle (in degrees) and dot product of 2 vectors\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(v1,v2)\n",
    "    cos_theta = dot_product/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return np.degrees(np.arccos(cos_theta)), dot_product\n",
    "v1 = np.array([1,2])\n",
    "v2 = np.array([-1,3])\n",
    "print(get_angle_dot_vectors(v1,v2))\n",
    "v2 = np.array([-1,0.5]) #perpendicular\n",
    "print(get_angle_dot_vectors(v1,v2))\n",
    "v2 = np.array([-1,0.2]) #theta > 90\n",
    "print(get_angle_dot_vectors(v1,v2))\n",
    "\n",
    "#Multidimentional\n",
    "v1 = np.random.rand(5)\n",
    "v2 = np.random.rand(5)\n",
    "print(\"Multidimentional vectors\", v1, v2))\n",
    "print(get_angle_dot_vectors(v1,v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lecture 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delta rule**\n",
    "\n",
    "$\\Delta w_i = -\\varepsilon x_i(t-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Derivation **\n",
    "\n",
    "Assume we have $N$ training samples and $W$ weight parameters.\n",
    "\n",
    "Our training samples are a matrix $x$ with element $x_{n,i}$ representing the $i$-th input value (column) for the $n$-th sample (row). $t$ represents a $N$-long vector containing the truth outcomes, while $y$ is a similarly $N$-long vector with the predicted outcomes.\n",
    "\n",
    "Our total model error $E = \\frac {1}{2} \\sum_{n \\in N} (t_n - y_n)^2$ and individual sample error $E_n = \\frac {1}{2}(t_n - y_n)^2$.\n",
    "\n",
    "We want to get the error $E$ to approach zero, thus we differentiate the error formula w.r.t. all weight parameters $W$. Since the same weight vector $W$ applies to all training cases, and each training case will yield a different partial differential for $W$, we can't in one go zero the error of all training cases.\n",
    "\n",
    "Instead we differentiate $E$, the sum of all errors on each weight parameter and substract the partial derivatives from the weight in the hope of minimizing the total error. If we do this for enough iteratios, we are guaranteed to reach a solution if a feasible solution exists. Convergence proof at http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial derivatives: For $i \\in W , \n",
    "\\frac{\\partial E}{\\partial w_i} = \\frac{1}{2} \\sum{n \\in N} \\frac{\\partial y_n}{\\partial w_i} \n",
    "\\frac{dE_n}{dy_n}$ using chain rule\n",
    "\n",
    "Since $\\frac{\\partial y_n}{\\partial w_i} = x_{n,i}$, by definition of $y_n$, and $\\frac{dE_n}{dy_n} = (t_n - y_n)$ by applying the product rule to the definition of individual sample error.\n",
    "\n",
    "Thus $\\frac{\\partial E}{\\partial w_i} = x_{n,i}.(t_n - y_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get $\\Delta w_i$, we use multiply the partial derivative $\\frac{\\partial E}{\\partial w_i}$ by both $-1$ and $\\varepsilon$. The $-1$ ensures that the new error value is less than the current one, while $\\varepsilon$, a positive real number, is called the *training rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How** does multiplying by $-1$ ensure error $E$ is decreasing?\n",
    "\n",
    "From definition of differentiation and for any $w$ weight vector and $x$ input vector, $\\frac{\\partial E}{\\partial w} \\approx \\frac{\\Delta E}{\\Delta w}$ for an infinitesimally small $\\Delta w = w - w_o$. Thus $\\frac{E-E_o}{w-w_o} \\approx (t-y).x$\n",
    "\n",
    "Assume $\\Delta w = -\\varepsilon.(t-y).x$, then $E-E_o = (\\varepsilon.(t-y).x).((t-y).x) = -\\varepsilon.(t-y)^2.x^2$, which is always negative. Thus, assuming a small value for $\\varepsilon$, adding the negative valued $\\Delta w$ to the current weights $w_o$, we will get the desired effect of lowering $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why** we use the learning rate $\\varepsilon$, and how to set it? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
