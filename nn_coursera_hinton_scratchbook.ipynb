{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron functions and feasible weight vectors\n",
    "*Lectures 1+2, GH*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear neurons**\n",
    "\n",
    "$y=b+\\sum_ix_iw_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(x) is (100,) and shape(w) is (100,)\n",
      "23.9605263108 ()\n"
     ]
    }
   ],
   "source": [
    "def linear_neuron(x,w,b = 0):\n",
    "    \"\"\"\n",
    "    x = (m*1)\n",
    "    w = (m*1)\n",
    "    b = scalar\n",
    "    \"\"\"\n",
    "    assert (len(x.shape) ==1) & (len(w.shape) == 1) & (x.shape[0] == w.shape[0])\n",
    "    print(\"shape(x) is %s and shape(w) is %s\" %(str(x.shape),str(w.shape)))\n",
    "    y = b + np.dot(w.T,x)\n",
    "    return y\n",
    "\n",
    "x = np.random.rand(100,)\n",
    "w = np.random.rand(100,)\n",
    "y = linear_neuron(x,w)\n",
    "\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(x) is (100,) and shape(w) is (100,)\n",
      "z is 27.390\n",
      "y is 0.000\n"
     ]
    }
   ],
   "source": [
    "def binary_threshold_neuron(x,w,b = 0, theta = None):\n",
    "    \"\"\"\n",
    "    x = (m*1)\n",
    "    w = (m*1)\n",
    "    b = scalar\n",
    "    \"\"\"\n",
    "    assert (len(x.shape) ==1) & (len(w.shape) == 1) & (x.shape[0] == w.shape[0])\n",
    "    if theta == None: theta = -b\n",
    "    print(\"shape(x) is %s and shape(w) is %s\" %(str(x.shape),str(w.shape)))\n",
    "    z = b + np.dot(w.T,x)\n",
    "    print(\"z is %.3f\" % z)\n",
    "    y = 1 if (z >= theta) else 0\n",
    "    return y\n",
    "\n",
    "x = np.random.rand(100,)\n",
    "w = np.random.rand(100,)\n",
    "y = binary_threshold_neuron(x,w, b=1, theta = 60)\n",
    "\n",
    "print(\"y is %.3f\" % y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(x) is (100,) and shape(w) is (100,)\n",
      "z is 0.021\n",
      "y is 0.5053624108087351185503167\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_neuron(x,w,b = 0.):\n",
    "    \"\"\"\n",
    "    x = (m*1)\n",
    "    w = (m*1)\n",
    "    b = scalar\n",
    "    \"\"\"\n",
    "    assert (len(x.shape) ==1) & (len(w.shape) == 1) & (x.shape[0] == w.shape[0])\n",
    "    print(\"shape(x) is %s and shape(w) is %s\" %(str(x.shape),str(w.shape)))\n",
    "    z = b + np.dot(w.T,x)\n",
    "    print(\"z is %.3f\" % z)\n",
    "    y = 1. / (1. + np.exp(-z))\n",
    "    return y\n",
    "\n",
    "x = np.random.rand(100,)\n",
    "w = np.random.rand(100,) * np.full(100,.001) #make weights small scale\n",
    "y = sigmoid_neuron(x,w)\n",
    "\n",
    "print(\"y is %.25f\" % y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUQy0P8HUutVd310rNIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4a63f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_sigmoid():\n",
    "    z = np.arange(-10,10,.02)\n",
    "    y = 1. / (1. + np.exp(-z))\n",
    "    plt.plot(z,y)\n",
    "    plt.show()\n",
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Angle between vectors**\n",
    "\n",
    "$A.B = |A|.|B|.cos(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45.000000000000007, 5)\n",
      "(90.0, 0.0)\n",
      "(105.25511870305778, -0.59999999999999998)\n",
      "Multidimentional vectors [ 0.08918746  0.55170278  0.52871369  0.6970892   0.61680078] [ 0.64628073  0.22057903  0.26132799  0.23215712  0.64651639]\n",
      "(43.546934955657242, 0.87810792158916384)\n"
     ]
    }
   ],
   "source": [
    "def get_angle_dot_vectors(v1,v2):\n",
    "    \"\"\"\n",
    "    Returns the angle (in degrees) and dot product of 2 vectors\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(v1,v2)\n",
    "    cos_theta = dot_product/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return np.degrees(np.arccos(cos_theta)), dot_product\n",
    "v1 = np.array([1,2])\n",
    "v2 = np.array([-1,3])\n",
    "print(get_angle_dot_vectors(v1,v2))\n",
    "v2 = np.array([-1,0.5]) #perpendicular\n",
    "print(get_angle_dot_vectors(v1,v2))\n",
    "v2 = np.array([-1,0.2]) #theta > 90\n",
    "print(get_angle_dot_vectors(v1,v2))\n",
    "\n",
    "#Multidimentional\n",
    "v1 = np.random.rand(5)\n",
    "v2 = np.random.rand(5)\n",
    "print(\"Multidimentional vectors\", v1, v2)\n",
    "print(get_angle_dot_vectors(v1,v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Neuron unit derivatives\n",
    "*Lecture 3, GH*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delta rule** for **linear** neuron\n",
    "\n",
    "$\\Delta w_i = -\\varepsilon x_i(t-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Derivation **\n",
    "\n",
    "Assume we have $N$ training samples and $W$ weight parameters.\n",
    "\n",
    "Our training samples are a matrix $x$ with element $x_{n,i}$ representing the $i$-th input value (column) for the $n$-th sample (row). $t$ represents a $N$-long vector containing the truth outcomes, while $y$ is a similarly $N$-long vector with the predicted outcomes.\n",
    "\n",
    "Our total model error $E = \\frac {1}{2} \\sum_{n \\in N} (t_n - y_n)^2$ and individual sample error $E_n = \\frac {1}{2}(t_n - y_n)^2$.\n",
    "\n",
    "We want to get the error $E$ to approach zero, thus we differentiate the error formula w.r.t. all weight parameters $W$. Since the same weight vector $W$ applies to all training cases, and each training case will yield a different partial differential for $W$, we can't in one go zero the error of all training cases.\n",
    "\n",
    "Instead we differentiate $E$, the sum of all errors on each weight parameter and substract the partial derivatives from the weight in the hope of minimizing the total error. If we do this for enough iteratios, we are guaranteed to reach a solution if a feasible solution exists. Convergence proof at http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial derivatives: For $i \\in W , \n",
    "\\frac{\\partial E}{\\partial w_i} = \\frac{1}{2} \\sum_{n \\in N} \\frac{\\partial y_n}{\\partial w_i} \n",
    "\\frac{dE_n}{dy_n}$ using chain rule\n",
    "\n",
    "Since $\\frac{\\partial y_n}{\\partial w_i} = x_{n,i}$, by definition of $y_n$, and $\\frac{dE_n}{dy_n} = (t_n - y_n)$ by applying the product rule to the definition of individual sample error.\n",
    "\n",
    "Thus $\\frac{\\partial E}{\\partial w_i} = x_{n,i}.(t_n - y_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get $\\Delta w_i$, we use multiply the partial derivative $\\frac{\\partial E}{\\partial w_i}$ by both $-1$ and $\\varepsilon$. The $-1$ ensures that the new error value is less than the current one, while $\\varepsilon$, a positive real number, is called the *training rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How** does multiplying by $-1$ ensure error $E$ is decreasing? Courtsey of http://users.sussex.ac.uk/~khs20/mtcs/mtcs/tutorial2.html\n",
    "\n",
    "From definition of differentiation and for any $w$ weight vector and $x$ input vector, $\\frac{\\partial E}{\\partial w} \\approx \\frac{\\Delta E}{\\Delta w}$ for an infinitesimally small $\\Delta w = w - w_o$. Thus $\\frac{E-E_o}{w-w_o} \\approx (t-y).x$\n",
    "\n",
    "Assume $\\Delta w = -\\varepsilon.(t-y).x$, then $E-E_o = (\\varepsilon.(t-y).x).((t-y).x) = -\\varepsilon.(t-y)^2.x^2$, which is always negative. Thus, assuming a small value for $\\varepsilon$, adding the negative valued $\\Delta w$ to the current weights $w_o$, we will get the desired effect of lowering $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why** we use the learning rate $\\varepsilon$, and how to set it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally we conclude that, the **batch delta** rule is $\\Delta w_i = -\\varepsilon \\sum_{n \\in N}x_{n,i}.(t_n - y_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error surface of neuron**\n",
    "\n",
    "requires simulation data, tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delta rule** for **logistic** neuron/unit\n",
    "\n",
    "For a logistic neuron where $y = \\frac{1}{1+e^{-z}}$ with $z = b+\\sum_i x_iw_i$\n",
    "\n",
    "$\\Delta w_i = -\\varepsilon x_i.y.(1-y).(t-y)$\n",
    "\n",
    "\n",
    "\n",
    "**Derivation**:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_i} = \\sum_{n \\in N}\\frac{\\partial y_n}{\\partial w_i} \\frac{dE_n}{dy_n} = \\sum_{n \\in N}\\frac{\\partial z_n}{\\partial w_i} \\frac{dy_n}{dz_n} \\frac{dE_n}{dy_n}$ using chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now work out all 3 components;\n",
    "\n",
    "- $\\frac{\\partial z_n}{\\partial w_i} = x_{n,i}$\n",
    "- $\\frac{dy_n}{dz_n} = y.(1-y)$, here is how;\n",
    "    - Since $y = (1+e^{-z})^{-1}$\n",
    "    - $\\frac{dy}{dz} = \\frac{-1.e^{-z}}{(1+e^{-z})^2}$, using power rule, $ = \\frac{1}{1+e^{-z}}.\\frac{e^{-z}}{1+e^{-z}} \\\\\n",
    "    = y.\\frac{(1+e^{-z})-1}{1+e^{-z}} \\\\\n",
    "    = y.\\frac{(1+e^{-z})}{1+e^{-z}}.\\frac{-1}{1+e^{-z}} \\\\\n",
    "    = y.(1-y)$\n",
    "- $\\frac{dE_n}{dy_n} = (t_n - y_n)$ by using the power rule\n",
    "\n",
    "Back to deriving our error function, $\\frac{\\partial E}{\\partial w_i} = x_{n,i}.y.(1 - y).(t_n - y_n)$ and for a batch weight correction, we use $\\Delta w_i = -\\varepsilon \\sum_{n \\in N}x_{n,i}.(t_n - y_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation error derivatives\n",
    "\n",
    "Assume the neural network has an output layer with $J$ neurons and hidden layer with $I$ neurons. The output value $y_i$, created by neuron $j$ activated by value $z_j$. $z_j$ is the *sigmoid* output of $I$ neurons layer with weights $w_{i,j}, i \\in I$. We also assume that the neuron $i$ is in the hidden layer where other neurons in layer $i$ feed into the neuron $j$ in a following layer. \n",
    "\n",
    "PUT PICTURE FROM SLIDE 28 of lec.3\n",
    "\n",
    "We want to figure out how fast the output error $E = \\frac{1}{2}\\sum_{j \\in J}(t_j - y_j)^2$ changes as we change the hidden weights $\\frac{\\partial E}{\\partial w_{i,j}}$, and how it changes in response to the output of the hidden neuron $\\frac{\\partial E}{\\partial y_i}$ . We will first work out the case for a single weight $w_{i,j}$, of the edge between the hidden neuroon $i$ and the subsequent neuron $j$, and its impact on $E$ through $y_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that $\\frac{\\partial z_j}{\\partial w_{i,j}} = y_i$ since $z_j = y_i.w_{i,j}$.\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w_{i,j}}$ \n",
    "$= \\frac{\\partial z_j}{\\partial w_{i,j}}.\\frac{\\partial E}{\\partial z_j} \\\\\n",
    " = y_i.\\frac{\\partial y_j}{\\partial z_j}.\\frac{\\partial E}{\\partial y_j} \\\\\n",
    " = y_i.y_j.(1-y_j).(t_j - y_j)$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial y_i}$\n",
    "$= \\sum_{j \\in J}\\frac{dz_j}{dy_i}.\\frac{\\partial E}{\\partial z_j} \\\\\n",
    " = \\sum_{j \\in J} w_{i,j}.\\frac{\\partial E}{\\partial z_j} \\\\\n",
    " = \\sum_{j \\in J} w_{i,j}.(y_j.(1-y_j).(t_j - y_j))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and choice\n",
    "\n",
    "Softmax allows us to constrain the *logistic* outputs of a neural network to represent a probability distribution across discrete alternatives\n",
    "\n",
    "Assume $i$ is the index of the alternative we are calculating from a $J$ set of alternatives.\n",
    "Softmax is defined as $y_i = \\frac{e^{z_i}}{\\sum_{j \\in J} e^{z_j}}$, with derivative $\\frac{\\partial y_i}{\\partial z_i} = y_i(1 - y_i)$, and $\\frac{\\partial y_i}{\\partial z_j} = -y_i. y_j$ when $ i\\neq j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(z) is (3,)\n",
      "[  9.99876603e-01   1.23394576e-04   2.06089928e-09]\n",
      "shape(z) is (3,)\n",
      "[ 0.64854841  0.26368011  0.08777148]\n",
      "shape(z) is (3,)\n",
      "[ nan  nan  nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrateb\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n",
      "C:\\Users\\jrateb\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    z = (m*1)\n",
    "    \"\"\"\n",
    "    assert (len(z.shape) ==1)\n",
    "    print(\"shape(z) is %s\" %str(z.shape))\n",
    "    exp_z = np.exp(z.T)\n",
    "    y = exp_z / sum(exp_z)\n",
    "    return y\n",
    "\n",
    "print(softmax(np.array([10,1,-10])))\n",
    "print(softmax(np.array([1,.1,-1])))\n",
    "print(softmax(np.array([1000,2000,5000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(z) is (3,)\n",
      "[ 0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "def softmax_shifted(z):\n",
    "    \"\"\"\n",
    "    z = (m*1)\n",
    "    \"\"\"\n",
    "    assert (len(z.shape) ==1)\n",
    "    print(\"shape(z) is %s\" %str(z.shape))\n",
    "    shifted_z = z - np.max(z)\n",
    "    exp_z = np.exp(shifted_z.T)\n",
    "    y = exp_z / sum(exp_z)\n",
    "    return y\n",
    "\n",
    "print(softmax_shifted(np.array([1000,2000,5000])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivation** of softmax\n",
    "\n",
    "Credits to https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "\n",
    "$\\frac{\\partial y_i}{\\partial z_j} = \\frac{\\partial \\frac{e^{z_i}}{\\sum_{k \\in K} e^{z_k}}}{\\partial {z_j}}$ \n",
    "\n",
    "We will use the quotient rule: For $f(x) = \\frac{g(x)}{h(x)}, f'(x)= \\frac{g'(x)h(x) - h'(x)g(x)}{(h(x))^2}$, and set $g(x) = e^{z_i}$ and $h(x) = \\sum_{k \\in K} e^{z_k}$. We will also use the fact that $\\frac{d(e^x)}{dx} = e^x$\n",
    "\n",
    "Also, note that the derivative of $g_i$ w.r.t. $z_j$ is $e^{z_j}$ only if $i = j$, otherwise, the derivative is $0$.\n",
    "\n",
    "First, we will derive the case where $i = j$: \n",
    "\n",
    "$\\frac{\\partial y_i}{\\partial z_j} = \\frac{\\partial \\frac{e^{z_i}}{\\sum_{k \\in K} e^{z_k}}}{\\partial {z_j}}$ \n",
    "$ = \\frac{e^{z_i} \\sum_{k \\in K} e^{z_k} - e^{z_j} e^{z_i}}{{\\sum_{k \\in K} e^{z_k}}^2} \\\\\n",
    " = \\frac{e^{z_i}}{\\sum_{k \\in K} e^{z_k}} \\frac{\\sum_{k \\in K} e^{z_k} - e^{z_j}}{\\sum_{k \\in K} e^{z_k}} \\\\\n",
    " = y_i(1-y_j)$\n",
    " \n",
    "Finally, if $i \\neq j$:\n",
    "\n",
    "$\\frac{\\partial y_i}{\\partial z_j} = \\frac{\\partial \\frac{e^{z_i}}{\\sum_{k \\in K} e^{z_k}}}{\\partial {z_j}}$ \n",
    "$ = \\frac{0 - e^{z_j} e^{z_i}}{{\\sum_{k \\in K} e^{z_k}}^2} \\\\\n",
    " = - \\frac{e^{z_j}}{\\sum_{k \\in K} e^{z_k}} \\frac{e^{z_i}}{\\sum_{k \\in K} e^{z_k}} \\\\\n",
    " = - y_j y_i$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
